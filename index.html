<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>KDQ&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Time waits for no one.">
<meta property="og:type" content="website">
<meta property="og:title" content="KDQ&#39;s Blog">
<meta property="og:url" content="https://qunniekong.github.io/index.html">
<meta property="og:site_name" content="KDQ&#39;s Blog">
<meta property="og:description" content="Time waits for no one.">
<meta property="og:locale" content="Chinese">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="KDQ&#39;s Blog">
<meta name="twitter:description" content="Time waits for no one.">
  
    <link rel="alternate" href="/atom.xml" title="KDQ&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">KDQ&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qunniekong.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Monocular-Depth-Estimation-7" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/28/Monocular-Depth-Estimation-7/" class="article-date">
  <time datetime="2018-05-28T02:27:30.000Z" itemprop="datePublished">2018-05-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/28/Monocular-Depth-Estimation-7/">Monocular Depth Estimation 7</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<h3 id="单目图像深度估计-相对深度篇：Depth-in-the-Wild-amp-Size-to-Depth"><a href="#单目图像深度估计-相对深度篇：Depth-in-the-Wild-amp-Size-to-Depth" class="headerlink" title="单目图像深度估计 - 相对深度篇：Depth in the Wild &amp; Size to Depth"></a>单目图像深度估计 - 相对深度篇：Depth in the Wild &amp; Size to Depth</h3><p>目前单目图像深度估计需要面临的主要问题之一就是我们用来获得ground truth depth的硬件设备本身具有一定误差和环境限制，比如说基于红外的相机(Kinect)无法在室外使用，而所有设备都只在一定距离范围内具有精确度，超出这个范围的获取结果是不可信的，因此我们所获得的深度图本身就具有一定误差。在误差的基础上进行建模，使得后续模型拟合的难度加大了。<br>因此就有科学家提出，其实人类对深度的实际数值并不敏感，反而是对相对深度即物体的前后关系更加敏感。这篇笔记打算写的两篇论文都是基于这个思考而来：<br>[1] Single-Image Depth Perception in the Wild, NIPS, 2016  [<a href="http://www-personal.umich.edu/~wfchen/depth-in-the-wild/" target="_blank" rel="noopener">Project Page</a>]<br>[2] Size to Depth: A New Perspective for Single Image Estimation, CVPR, 2018</p>
<hr>
<h4 id="Single-Image-Depth-Perception-in-the-Wild-NIPS-2016"><a href="#Single-Image-Depth-Perception-in-the-Wild-NIPS-2016" class="headerlink" title="Single-Image Depth Perception in the Wild, NIPS, 2016"></a>Single-Image Depth Perception in the Wild, NIPS, 2016</h4><p>这篇文章很有趣，作者指出就算是具有完备又复杂视觉系统的人类也很难推断出物体的实际深度(距离)值，而且人也会被迷惑(比如“鸽子为什么这么大”)，但人可以根据经验(物体大小)、遮挡关系、光线和阴影等知识来准确的判断相对深度，即物体的前后关系。之前已经有由相对深度估计绝对深度的研究，但缺少适用于现状的数据集。因此这篇文章的贡献主要有：</p>
<ul>
<li>Depth in the Wild数据集</li>
<li>通过相对深度预测绝对深度的方法</li>
</ul>
<p><strong>DIW数据集</strong><br>目前已有的数据集的缺陷有：基于Kinect的数据集局限于室内场景；基于LIDAR的数据集局限于人造的场景(街道)。因此DIW的数据集的目的是采集更多的非人造场景。<br>文章作者详细写了数据集的整理和标注方法。首先，从英文词典中随机抽取关键词到图片网站Flickr上搜索，然后手动去除剪贴画等人造的图片，最后用众包的方式获取打标结果。<br>数据集中相对深度的表达方式为问询(query)，一个query可以表达为(i,j,r),其中i，j为两个数据点，r为两点像素的相对深度，0表示难以判断的接近，+1表示i比j更近，-1表示i比j更远。<br>为了更有效的利用人为的打标信息，数据集中每幅图片选取一对像素点进行一次问询。作者解释说，在一幅图片上选取多个点获取其相对深度是可行的，但存在问题是往往相似的点具有相似的深度，因此在一幅图上标记多点难免产生数据冗余。<br>文章还对如何选取这唯一的一对像素点进行了探讨。有趣的实验结果如下：</p>
<ul>
<li>直接判定<strong>更靠近图片底部</strong>的点为较近点可以获得85.8%的预测正确率。</li>
<li>同一水平线上的两个点，直接判定<strong>更靠近中心点</strong>的点为较近的点可以获得71.4%的预测正确率。<br>因此本文作者选取的方式为：选取同一水平线上距离中心点距离相同(左右对称)的两个点。用这样的选取方式选取的两个点，左边的点为较近点的概率为50.3%。</li>
</ul>
<p><strong>通过相对深度预测绝对深度</strong><br>这部分也比较有意思，以我的理解，作者的意思是说假如数据集足够大只需要一个能够拟合相对深度的网络就可以预测绝对深度。是不是说一幅图片如果相对深度能够确定，自然可以确定绝对深度呢？<br>作者提出的Loss缺失没有依赖绝对深度，而是只根据相对深度。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth7_diw_eq.png" alt="Loss"><br>上图为文章提出的Loss，若两个点的相对深度关系为“接近”时Loss让两个点的绝对深度更加接近，反之则让两个点的绝对深度之间具有较大差距。<br>通过实验发现先在NYU Depth数据集上训练然后在DIW数据集上进行Refine得到的网络具有最优的预测表现。</p>
<hr>
<h4 id="Size-to-Depth-A-New-Perspective-for-Single-Image-Estimation-CVPR-2018"><a href="#Size-to-Depth-A-New-Perspective-for-Single-Image-Estimation-CVPR-2018" class="headerlink" title="Size to Depth: A New Perspective for Single Image Estimation, CVPR, 2018"></a>Size to Depth: A New Perspective for Single Image Estimation, CVPR, 2018</h4><p>上一篇文章有一个主要的问题就是同样的图片可能会对应完全不同的距离值，假设同一部相机在不同距离拍摄相同场景的情况。这篇文章针对这种问题，提出size to depth的方法。<br>文章中提到了两个很有趣的实验：</p>
<ul>
<li>从2D图片判断绝对深度：被试验者犹豫的时间较长，且结果有21%的相对误差。</li>
<li>从2D图片中判断物体尺寸： 被试验者犹豫的时间较短，且结果只有8%的相对误差。<br>我个人理解，犹豫时间可以表达人类在这方面能力的强弱，因此人类由于先验知识的影响，判断物体尺寸的能力远远好于判断距离的能力。</li>
</ul>
<p>Size to Depth，根据画面内物体的size信息推测深度。具体过程首先是把图片分割成小块(pitch)为每个pitch设定一个size，这个size由人工标注，size的值为pitch中的主要物体(dominant component)的实际大小，最后用CRF进行平滑。<br>CRF的研究不算多，可能是因为CRF的复杂度比较高而且会影响到输出size的原因。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth7_size_eq.png" alt="Loss"><br>上面公式为文章使用的energy function，其中第一部分的目标是缩小预测值与标记值的差异，第二部分是consistency约束。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/" data-id="cjhpuzabd000heg8utusbrr2l" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Monocular-Depth-Estimation-6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/27/Monocular-Depth-Estimation-6/" class="article-date">
  <time datetime="2018-05-27T06:34:01.000Z" itemprop="datePublished">2018-05-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/27/Monocular-Depth-Estimation-6/">Monocular Depth Estimation 6</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<h3 id="单目图像深度估计-6-无监督篇：Left-Right-Consistency-amp-Ego-Motion"><a href="#单目图像深度估计-6-无监督篇：Left-Right-Consistency-amp-Ego-Motion" class="headerlink" title="单目图像深度估计 - 6. 无监督篇：Left-Right Consistency &amp; Ego Motion"></a>单目图像深度估计 - 6. 无监督篇：Left-Right Consistency &amp; Ego Motion</h3><p>近几年有关单目图像深度识别的算法以CNN为主流，更细的说是以无监督的同时对深度、计算机角度、光流等同时计算的端到端深度网络为主流。<br>所谓无监督其实是指在训练过程中不需要输入真实的深度值，这样做有一个好处就是目前能够测量到深度信息的传感器还不够精确，因此由不够精确的label训练出的model得到的预测结果必然不会特别令人满意；<br>所谓同时计算呢，在我理解是指在训练过程中，用一个能够表征时间序列上有前后关系的帧之间的差别的loss同时训练多个网络，而在得到model后每个网络可以单独使用。<br>很聪明，不同作用的网络相当于人为的特征提取过程，最后的预测基于这个人为的特征提取结果，但这种方法也有其缺点，我能想到的就是参数的增加，网络结构的复杂化和人为特征对最终预测结果有没有起引导作用只能用实验去证明。</p>
<p>详细说呢，首先，所谓的“无监督”虽然不需要输入真实深度信息，但需要输入双目摄像头获取到的同一时刻不同角度的图像或者前后帧图像，只是这样就叫做无监督在我看来略显牵强。<br>其次，关于多网络共同训练，本来深度网络就很难解释，复杂化网络的结构得到多个看似可以解释的子网络，是否和深度网络的端到端黑盒特性有所冲突？较重的人为干涉是不是反而影响深度网络对数据隐含知识的理解和抽取？<br>以上只是我个人的一些思考，希望在未来的学习过程中能得到一些答案。</p>
<hr>
<h4 id="UnSupervised-Learning-of-Depth-and-Ego-Motion-from-Video，CVPR，2017"><a href="#UnSupervised-Learning-of-Depth-and-Ego-Motion-from-Video，CVPR，2017" class="headerlink" title="UnSupervised Learning of Depth and Ego-Motion from Video，CVPR，2017"></a>UnSupervised Learning of Depth and Ego-Motion from Video，CVPR，2017</h4><p>接下来写一下Google发表于CVPR2017的这篇文章，从题目可以看出这篇文章提出了一种非监督的多功能网络，主要思想就像之前提到过的用一个loss同时训练两个网络。网络的结果如Fig.2,其中第一个网络可接受一幅图片作为输入，输出其对应的深度图片；第二个网络为姿态网络，接受t，t+1和t-1三个时刻三幅图片作为输入，输出从t到t+1和从t到t-1的相机姿态变化矩阵。<br>关于Pose的部分我不很了解，所以主要说明一下Depth CNN网络和Loss的结构。</p>
<p><strong>Depth CNN</strong><br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth6_ego_model1.png" alt="Model Architecture"><br>基本结构见上图，输入为前中后三帧连续的图片，同时训练两个网络，一个得到深度预测结果，一个得到视差矩阵结果。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth6_ego_model2.png" alt="Model Architecture"><br>其中视差网络用到了深度预测网络的预测结果。应用结构与DisNet相同的网络作为深度估计的网络，DispNet拥有主流的encoder-decoder结构，下一步打算看一下DispNet的相关Paper，因此在这不多介绍。作者提到使用多视角的图片训练深度预测网络结果和单张图片效果没有很大差异，说明光流约束需要在对多视角图片进行有效利用的前提下使用。</p>
<p><strong>Loss</strong><br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth6_ego_eq.png" alt="Loss"><br>由Eq.4可见，Loss分为三个部分。其中第一部分为sample和target的差别，第二部分为多尺度平滑参数，第三部的目的是为了避免Es趋于0。</p>
<hr>
<h4 id="UnSupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency-CVPR-2017"><a href="#UnSupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency-CVPR-2017" class="headerlink" title="UnSupervised Monocular Depth Estimation with Left-Right Consistency, CVPR, 2017"></a>UnSupervised Monocular Depth Estimation with Left-Right Consistency, CVPR, 2017</h4><p>这篇Paper主要思想为使用双目摄像头得到的同一时刻的两幅图片(left，right)进行训练，得到由left生成right（或right生成left）的网络，然后根据生成的双目图片得到depth。那所谓的无监督是指不需要ground truth depth，只需要双目图片。这种方法的好处是避免了深度测量硬件本身的误差，作者提出现有的深度测量硬件比如雷达、红外相机、TOF相机等本身就有误差，而且具有有效范围的限制，对比看来摄像头或双目摄像头硬件技术更为成熟，误差也会更小，因此在此基础上训练出来的网络应该有更好的精度。</p>
<p>目前也有类似的由左图生成右图的方法，但本位方法的改进就在于提出了一种left-right consistency，在训练过程中不仅限制由左图到右图的连续性，同时也限制右图到左图的连续性。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth6_lr_comp.png" alt="Method Compare"><br>上图为几种方法的比较，可以看出Naive方法的输出只受target影响，而NoLR方法的输出受左右图同时的影响，本文方法则在NoLR的基础上增加了左右连续性限制。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth6_lr_eq.png" alt="Loss"><br>上图为文章的主要创新点：左右一致性Loss。这个Loss可以同时考虑到左右视差一致性、平滑性、重建效果。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth6_lr_loss.png" alt="Loss Architecture"><br>Loss公式表示如上图，可见Loss分为三部分：</p>
<ul>
<li>第一部分为与input的相似性</li>
<li>第二部分为平滑性约束</li>
<li>第三部分为左右一致性约束</li>
</ul>
<p>CNN网络的结构夜视基于DispNet在此不再说明。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这两篇文章都是基于无监督的方法，但说是无监督又有点牵强，但我认为无论在那个研究领域，无监督都是最终的目标，毕竟label总有不可靠的概率，自我学习和纠正能力才是人工智能具有智能的真正标志。<br>两种方法都用到了DispNet，是不是可以说明现有的CNN模型结构完全可以胜任大部分计算机视觉任务呢？<br>很多CV相关的Paper中的网络结构总结起来有以下几种情况：</p>
<ul>
<li>使用已有网络训练好的参数初始化</li>
<li>使用已有网络的结构</li>
<li>使用已有网络中的一部分<br>其中已有网络指VGG,ResNet,DispNet,LeNet等等。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/" data-id="cjhpuzab9000feg8ubeb3at6o" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Monocular-Depth-Estimation-5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/22/Monocular-Depth-Estimation-5/" class="article-date">
  <time datetime="2018-05-22T03:12:20.000Z" itemprop="datePublished">2018-05-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/22/Monocular-Depth-Estimation-5/">Monocular Depth Estimation 5</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<h3 id="单目图像深度估计-5-深度篇：David-Eigen的两篇研究"><a href="#单目图像深度估计-5-深度篇：David-Eigen的两篇研究" class="headerlink" title="单目图像深度估计 - 5. 深度篇：David Eigen的两篇研究"></a>单目图像深度估计 - 5. 深度篇：David Eigen的两篇研究</h3><p>终于写到了目前比较主流的深度学习方法。随着大规模的数据集的出现以及硬件运算能力的提高，数据驱动的方法开始在计算机视觉、自然语言理解等领域发光发热。David Eigen可以说是第一个把深度学习方法用于单目图像深度估计的人，他提出的多尺度CNN网络到现在仍然在被引用和对比。这篇笔记就写一下Eigen的两篇经典论文：<br>[1] Depth Map Prediction from a Single Image using a Multi-Scale Deep Network，NIPS 2014. <a href="https://cs.nyu.edu/~deigen/depth/" target="_blank" rel="noopener">Project Page</a><br>[2] Predicting Depth, Surface, Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture，ICCV 2015 <a href="https://cs.nyu.edu/~deigen/dnl/" target="_blank" rel="noopener">Project Page</a><br>从题目可以看出，第一篇Paper侧重于单目图像深度估计的任务中多尺度深度网络的应用，第二篇Paper主要内容为提出一个通用的网络可以解决单目深度估计、语义分割和法向量预测三个Task。从网络结构来看，第二篇提出的网络是在第一篇Paper中提出的网络的基础上进行的微调，同时也对loss进行了小小的调整，但整体改动不大。因此这篇阅读笔记也重点记一下第一篇Paper。</p>
<h4 id="Depth-Map-Prediction-from-a-Single-Image-using-a-Multi-Scale-Deep-Network"><a href="#Depth-Map-Prediction-from-a-Single-Image-using-a-Multi-Scale-Deep-Network" class="headerlink" title="Depth Map Prediction from a Single Image using a Multi-Scale Deep Network"></a>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</h4><hr>
<p>从2维推导出3维其实是一个ill-posed的Task，因为在拍摄照片即从三维（现实世界）到二维（照片）的过程中，信息的丢失是不可逆的。然而在计算机视觉相关的任务中，深度信息的增加可以提高相应算法的识别率，比如图像分割、识别等，而这些计算机视觉方法所代表的能力（比如环境感知）往往是机器人等热门AI技术所需要的。因此能否利用大规模的已有数据去训练深度网络使其可以对缺失的三维信息进行估计的研究具有很重要的现实意义。</p>
<h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><p>文章中提出了一个多尺度的混合网络，网络具有两个部分，分别为：</p>
<ol>
<li>一个全局粗网络(coarse-scale network):在全局的level对整幅图像的深度进行初步预测，全局信息通过全连接层实现，粗网络得到的结果分辨率较小，因此需要进一步优化。</li>
<li>一个局部精网络(fine-scale network):在粗网络的预测结果基础上，结合原始输入图片的局部信息对预测结果进行进一步优化。<br>网络结构如下图所示：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_depth_map_model.png" alt="Model Architecture"></li>
</ol>
<p>第一部分为全局粗网络，深度估计与其他Task的一个区别就是全局性，人类在确定深度的时候就用到了很多全局特征，比如消失点、物体位置、遮挡关系等等（这些在<a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇</a>中有相关介绍），而其他Task比如人脸识别，可能更重要的是找到某一局部的特征。因此在全局的层面上对深度进行预测是比较重要的一个环节。<br>全局粗网络包括7层，其中前5层为卷积和最大池化层，后两层为全连接层。输入图像的尺寸为304X228（NYU_Depth_v2图像的原始尺寸为640X480，此处先对图像进行了参数为2的下采样，然后裁剪掉了部分边界），输出图像的尺寸为74X55，即原图的四分之一。<br>除了粗精度网络的最后一层使用线性函数，所有隐藏层都使用Relu（rectified linear units）激活。其中有一些tricks，比如引用了一种自学习upsampling，可自主学习图像边界；粗网络和细网络的训练不同步进行，训练好了粗网络后将参数固定再训练细网络；在Layer6有dropout操作，1到5层使用在ImageNet数据集上预训练得到的参数进行初始化等。</p>
<p>第二部分为细网络，其中fine layer1进行了池化操作形成74X55大小的特征图stack，这个大小正好和粗网络的输出一致，因此在fin layer2可以将粗网络的输出作为一个feature map输入。为保持图像大小不变，后续的两层卷积层都进行的是0填充(zero-padded,用0填充边界使卷积后的结果大小不会改变)卷积。</p>
<p>在网络训练的过程中，用到了数据增加和异常点排除的Tricks。</p>
<ul>
<li>Data Augmentation：这是数据驱动的深度学习方法都需要的一步，由于深度网络的训练过程需要大量的数据，越大越好，因此对已有的数据进行缩放、旋转、随机裁剪、色彩改变、翻转等操作，可有效增加训练数据量，提高网络识别正确率。</li>
<li>在NYU_Depth数据集中，由于硬件的原因，部分数据点的深度数据是缺失的，作者使用mask来标记这些缺失的点，并且采用直接下采样的方法使缩小尺寸后的图像和mask仍然具有对应关系，因此可以在训练过程总过滤掉数据缺失的点。</li>
</ul>
<h5 id="Scale-Invariant-Error-amp-Training-Loss"><a href="#Scale-Invariant-Error-amp-Training-Loss" class="headerlink" title="Scale-Invariant Error &amp; Training Loss"></a>Scale-Invariant Error &amp; Training Loss</h5><p>文章的另一创新点为尺度不变损失函数，公式如下：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_depth_map_error1.png" alt="eq1"><br>其中：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_depth_map_error3.png" alt="eq2"><br>对任意尺度的深度预测结果y及对应的真实深度图y*来说，得到的error是一样大的，因此体现其尺度不变性。如果用d表示log空间中预测值和实际值的差异，上述公式可写作：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_depth_map_error2.png" alt="eq3"><br>由此可得到Training Loss如下，其中参数λ为0到1之间的值，当λ等于0时l2error不对loss进行影响：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_depth_map_loss.png" alt="eq4"></p>
<p>以上就是NIPS 2014这篇Paper的主要内容，在实验部分，作者在NYU和KITTI两个数据集都进行了实验，取得了比较好的结果。但这种方法存在以下缺点：</p>
<ol>
<li>输出的预测结果很小，只为原始输入图像的四分之一</li>
<li>边界仍然不够清晰，且四个角点有明显的错误点</li>
<li>在室内数据集上训练得到的网络只适用于室内图片的预测<br>以上三点也是大部分深度方普遍存在的缺点。</li>
</ol>
<h4 id="Predicting-Depth-Surface-Normals-and-Semantic-Labels-with-a-Common-Multi-Scale-Convolutional-Architecture"><a href="#Predicting-Depth-Surface-Normals-and-Semantic-Labels-with-a-Common-Multi-Scale-Convolutional-Architecture" class="headerlink" title="Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture"></a>Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture</h4><hr>
<p>这篇文章的主要创新点在于用一个结构通用（不同任务只需要改动参数）的网络可进行三种不同的Task并且得到令人满意的结果。但我们的关注点在单目图像深度估计这一部分，因此关于法向量估计和图像语义分割的部分就不写在这篇笔记中了。<br>这篇文章的网络结构在上一篇基础上进行了一些改进，但先粗后细的思想仍然没变，改进后的网络结构如下图：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_dnl_model.png" alt="Model Architecture"><br>由图可见，网络结构由两部分变为了三部分，并且网络的层数有所增加。与文献[1]不同的是：</p>
<ol>
<li>增加了Scale3使输出图片的大小扩大至输入图像的二分之一。</li>
<li>新提出的网络(DNL)中，coarse网络不再将预测结果传输给fine网络，而是将多通道的feature maps传输给fine网络，使得coarse网络和fine网络可以同时训练。</li>
<li>粗网络提出了两种不同的网络结构（AlexNet和VGG），<br>Training Loss也有相应的小改动，改动后的loss如下：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth5_dnl_loss.png" alt="Training Loss"><br>在尺度不变Loss的基础上增加了图像梯度的部分，这部分可使得在预测值和实际值接近的基础上其趋势也要接近。<br>在训练的过程中，对coarse和fine网络同时进行训练，然后固定其参数训练scale3的网络。</li>
</ol>
<p>从实验结果来看，改进后的网络在预测正确率和输出图片质量(大小)上均有所提高，同时VGG的效果好于AlexNet，因此作者提出在深度学习方法中，网络size对预测结果有很重要的影响。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/" data-id="cjhpuzabs000neg8u55ar4kiq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Monocular-Depth-Estimation-4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/06/Monocular-Depth-Estimation-4/" class="article-date">
  <time datetime="2018-03-06T03:19:36.000Z" itemprop="datePublished">2018-03-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/06/Monocular-Depth-Estimation-4/">Monocular Depth Estimation 4</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<h3 id="单目图像深度估计-4-迁移篇：Depth-Extraction-from-Video-Using-Non-parametric-Sampling"><a href="#单目图像深度估计-4-迁移篇：Depth-Extraction-from-Video-Using-Non-parametric-Sampling" class="headerlink" title="单目图像深度估计 - 4. 迁移篇：Depth Extraction from Video Using Non-parametric Sampling"></a>单目图像深度估计 - 4. 迁移篇：Depth Extraction from Video Using Non-parametric Sampling</h3><p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_paper.png" alt="Depth Extraction from Video"><br>第四篇写一下<a href="https://kevinkarsch.com/?portfolio=300" target="_blank" rel="noopener">Depth Extraction from Video Using Non-parametric Sampling</a>这篇文章中的Depth Transfer方法。</p>
<p>不同于其他主流方法，Depth Transfer并没有训练出特定的识别模型，而是通过把有标签数据与待预测样本进行点到点的对应，然后将深度信息进行迁移，形成深度估计结果。虽然诸如处理时间长、受训练集影响大等缺点十分显而易见，但这种思路仍然让人眼前一亮。其实如何能更好的利用有标签数据也是值得研究的方向，尤其是现在，深度网络的可解释性不高，我们不能确定拿到黑盒子里去训练一定能对宝贵的有标签数据进行有效利用，那不如换个方法。</p>
<p>言归正传，文章中提到Depth Transfer方法既能应用于单幅图像的深度估计，也能应用于视频的深度估计。显然，视频比图像多了时间前后文关系，因此文章的亮点就是如何利用有标签数据集和视频的前后文关系对图像的深度进行估计。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_transfer.png" alt="Depth Transfer"><br>Fig.1</p>
<p>如Fig.1所示，算法主要分为三个步骤：</p>
<ol>
<li>在RGBD数据集中寻找与input相似的多幅图片</li>
<li>通过将选出的相似图片(candidates)形变使其与input对应，得到input的初步深度估计结果</li>
<li>对step2的结果进行全局优化，得到最终pixel level的深度估计</li>
</ol>
<h4 id="图像深度估计"><a href="#图像深度估计" class="headerlink" title="图像深度估计"></a>图像深度估计</h4><p>相似的场景会对应相似的深度，Depth Transfer方法建立在这个基础之上。</p>
<p><strong>candidate的选取</strong>：</p>
<ol>
<li>首先，计算input及有标签数据集中每幅图像(或视频中的每帧图像)的GIST特征和光流特征信息；</li>
<li>然后，将input与数据集中的图像进行比较，选取K(=7)个最接近的匹配作为candidate。在这个过程中，限制数据集中同一video内的帧只能选取一个，以保证选出来的candidate有一定的变化；</li>
<li>最后，调用SIFT flow算法将input与candidate进行像素级的对应。SIFT flow算法为每个candidata产生一个对应方法(wraping function),wraping function将candidate的像素位置与input的像素位置相对应。</li>
</ol>
<p><strong>优化深度估计</strong>：<br>通过选取和wraping，为input产生了K(candidate个数)个像素级的深度估计结果，接下来讲解如何利用所有预测结果对深度估计进行优化。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq1.png" alt="Eq1"><br>Eq.1</p>
<p>Eq.1为Depth Transfer方法的Loss，通过最小化Loss可以得到最优估计。其中L为input图像，D为深度估计结果，Z为概率归一化常数，alpha=10，beta=0.5。<br>E(D)包含三部分，其中Et表示数据特性，Es表示平滑特性，Ep表示数据集特性。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq2.png" alt="Eq2"><br>Eq.2</p>
<p>Et可用来衡量深度估计D与每个wrap后的candidate深度的相似性,其中Phi为L1范式。公式第二部分是为了最小化x,y两个方向的梯度差异。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq3.png" alt="Eq3"><br>Eq.3<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq4.png" alt="Eq4"><br>Eq.4</p>
<p>Eq.3和Eq.4分别为Es和Ep的计算方法，其中Prior P为数据集中全部图像的均值。</p>
<h4 id="视频深度估计"><a href="#视频深度估计" class="headerlink" title="视频深度估计"></a>视频深度估计</h4><p>视频比单幅图像多了时间前后文关系，因此在对视频的深度估计进行优化时，在Eq.1的基础上增加了Ec和Em两部分，以保证：</p>
<ol>
<li>物体深度在时间上是连续的；</li>
<li>运动物体的深度与其接地点一致。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq5.png" alt="Eq5"><br>Eq.5<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq6.png" alt="Eq6"><br>Eq.6<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth4_eq7.png" alt="Eq7"><br>Eq.7</p>
<p>其中，Ec为时序关系，Em为运动线索。通过计算同一时间序列上有前后关系的每两张图片的光流变化控制其时序连续性。<br>同时，对视频中的运动物体进行检测提取并对其进行接地约束（运动物体与地面接触），在此过程中通过判断pixel和背景的差别来检测运动的物体。</p>
<h4 id="总结和思考"><a href="#总结和思考" class="headerlink" title="总结和思考"></a>总结和思考</h4><p>Depth Transfer的缺点是处理时间较长（每帧图片需要1分钟）并且受有标签数据集影响大（当数据集包含较多图像时搜索时间成倍增长），优点是对运动物体的深度估计有很好的效果，因此比较适用于电影2D转3D等主要物体比较明确的场景。<br>这篇文章非常长，因为包括了方法的介绍、数据集的介绍以及非常丰富的实验对比，还有很长的附加文档。之所以挑出这篇来写，是因为我在那么多深度网络的图像处理论文里总觉得自己要迷失了，总是思考一直对一个黑盒子进行微调到底是不是正确的方法，而这片文章正好给了一个全新的思路。有时候规律是潜藏的，我们没有发现之前总觉得它不存在，假若我们的生活场景能够分解成特定的几种模式的话，只要用比较简单的寻找-对应就可以进行一切估计了。</p>
<p>[1] Karsch K, Liu C, Kang S B. Depth Extraction from Video Using Non-parametric Sampling[C]// European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2012:775-788.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/" data-id="cjhpuzab7000deg8u4td70fzs" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Monocular-Depth-Estimation-3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/22/Monocular-Depth-Estimation-3/" class="article-date">
  <time datetime="2018-02-22T08:49:43.000Z" itemprop="datePublished">2018-02-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/22/Monocular-Depth-Estimation-3/">Monocular Depth Estimation 3</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<h3 id="单目图像深度估计-3-尺度篇：Make3D"><a href="#单目图像深度估计-3-尺度篇：Make3D" class="headerlink" title="单目图像深度估计 - 3. 尺度篇：Make3D"></a>单目图像深度估计 - 3. 尺度篇：Make3D</h3><p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth3_results.png" alt="Make3D"></p>
<p>终于进行到第三篇，这次膜拜一下单目图像深度识别的经典方法：Make3D(<a href="http://make3d.cs.cornell.edu/index.html" target="_blank" rel="noopener">Website</a>)。 Make3D方法经典的原因有二，其一是相关论文<a href="http://www.cs.cornell.edu/~asaxena/learningdepth/NIPS_LearningDepth.pdf" target="_blank" rel="noopener">Learning Depth from Single Monocular Images</a>发表于2005年，是我找到的最早的一篇单目图像深度识别的论文；其二是近期的许多研究在论文中都将自己的方法与Make3D进行了对比。这篇博客主要整理一下2005年发表于NIPS的<a href="http://www.cs.cornell.edu/~asaxena/learningdepth/NIPS_LearningDepth.pdf" target="_blank" rel="noopener">这篇</a>文章内容和我的相关理解。</p>
<p>上一篇文章中提到过，想要对深度进行估计，不能仅仅依靠局部特征(local features)还需要全局信息(global context)。举个例子，一个蓝色的像素点或一小块蓝色的图像区域，仅仅依靠其本身的信息无法判断它是属于一个蓝色的物体还是属于蓝色的天空。文中提到，人类对深度进行预测时也用到了全局特征如纹理变化，结构变化，轮廓遮挡，已知物体大小，模糊和失焦等。因此，想进行更加准确的深度预测，重要的是<strong>如何提取并利用全局信息</strong>。</p>
<h4 id="多尺度图像"><a href="#多尺度图像" class="headerlink" title="多尺度图像"></a>多尺度图像</h4><p>为什么多尺度信息可以作为全局信息应用呢？这就需要说一下什么是尺度以及图像的多尺度性质了。以下引用于互联网：</p>
<p><em>在一幅图像中，只有在一定的尺度范围内，一个物体才有意义。举一个例子，树枝这个概念，只有在几厘米到几米的距离去观察它，才能感知到它的确是树枝；如果在微米级或者千米级去观察，就不能感知到树枝这个概念了，这样的话可以感知到的是细胞或者是森林的概念。因而，如果想要描述现实世界的结构，或者将三维物体映射到二维的图像上去，多尺度表示将会至关重要。多尺度表示的概念很容易理解，举例说明，绘制地图时会有比例尺的概念。世界地图中就只能够显示大洲大洋，以及较大的地域和国家；而一个城市地图，甚至可以详细的显示出每条街道。</em></p>
<p><em>图像的多尺度性质，类似于人的眼睛观察物体，在离物体的距离不同时，所感知的特征是不一样的。也就是同一物体在视场中，成像的大小不同时，也就是尺度不同时，表现出的特征不相同。</em></p>
<p>读研时候接触过遥感图像的多尺度分割，在我理解，大的尺度分割即将图像分为数量较少的大面积区域，小的尺度分割即为将图像划分成数量较多的小面积区域。因此，若是一副图像包含大海、沙滩和一把放置在沙滩上的躺椅，在某一较大尺度将图像分为两部分，分别为沙滩和海面，那么在这一尺度上无法找到躺椅；同理，在某一较小尺度下，将图像分为躺椅、多块海浪、平静海面、背光沙滩、受光沙滩等好几部分，则无法获得完整的大海区域。扯一句题外话，多尺度分割的一个优势是存在一种分级结构，即总是在分割结果的基础上进行合并或进一步分割，因此分割区域间不存在重叠，使得后续处理和计算可以更快速。</p>
<p>在图像的高斯金字塔或降采样形成的金字塔中，由顶到底尺度减小，因此在高斯金字塔的顶端图线上提取到的信息可视作全局信息。举个例子，我们取一个图像区域相邻的上、下、左、右四个区域作为其邻居，这四个邻居的像素均值作为此区域的一个特征，那么在原始图像上这五个图像区域可能都为某一物体的一部分，而在高尺度的图像上，相邻的区域可能为不同的物体。</p>
<h4 id="Make3D方法"><a href="#Make3D方法" class="headerlink" title="Make3D方法"></a>Make3D方法</h4><p>Make3D方法中，将局部信息和多尺度全局信息组合成特征向量，然后应用MRF方法进行深度估计。选取MRF方法是因为MRF适用于局部特征不够需要用到全局特征的情况，MRF的常见应用有物体识别、文本分割、图像打标等。</p>
<p>其中局部信息和全局信息用不同空间尺度(scales)即图像分辨率(image resolution)上的特征表示。Make3D方法还创新的提出了一种列特征(Column Feature),由于室外图像中大部分物体如树、楼房等都具有竖直的结构，即物体应“站立”在地面上而不是悬浮在空中，因此文章选取了每个小区域(patch)竖直方向上下多个相邻区域计算其列特征。</p>
<p>文章将图像划分成多块(patch)，并将每一块的特征分为绝对深度特征(Absolute Depth Feature)和相对深度特征(Relative Depth Feature)。其中，绝对深度特征是指单个patch的深度，相对深度特征是指两个patch之间的差异。绝对深度特征的获取方法如下图：</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth3_feature%20vector.png" alt="Feature Vector"><br>Fig.2 Absolute depth feature</p>
<p>Fig.2的上部分为Make3D方法使用的卷积mask，从左到右，前9个为Law’s mask(Law’s texture energy in TEXTURE)其中第一个用于提取模糊特征；后六个用于边界检测。Fig.2的下半部分说明了特征向量的组成以及多尺度特征和列特征的图示说明。由图可见，单个patch的特征由三个不同尺度的特征和一个列特征组成，而patch间的相对特征由直方图的差异表示。因此，某个patch的特征向量为：((1+4)*3+4)*34=646维(自身加上四个邻居在三个尺度上的特征，加上4个列特征；17个Law’s mask结果的1次和2次能量计算)。另外，相对深度特征的计算方法为：每个patch计算17维的mask结果，并且由每一个结果图生成10列的直方图得到170个特征后做差计算。</p>
<p>得到特征向量后，构建概率模型并且通过求取最大后验概率得到模型参数。文章构建了两种模型分别为高斯模型(Fig.3)和拉普拉斯模型(Fig.4)。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth3_gaussian.png" alt="Gaussian"><br>Fig.3 Gaussian<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth3_laplacian.png" alt="Laplacian"><br>Fig.4 Laplacian</p>
<p>公式的前半部分可最小化预测值与实际深度值之间的差异，后半部分为平滑性约束即最小化不同尺度上相邻patch的差异。由于高斯模型无法生成边界清晰的深度预测结果，因此引入拉普拉斯模型。通过求解凸优化问题得到最优解，实现深度预测，Make3D方法的实验结果见下图。</p>
<p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth3_experiments1.png" alt="Experiment1"><br>Fig.5<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth3_experiments2.png" alt="Experiment2"><br>Fig.6</p>
<p>Fig.5和Fig.6为实验结果，Fig.5中从左到右分别为实际图像、实际深度、高斯结果和拉普拉斯结果,Fig.6为应用不同特征进行计算的结果对比，可以看出拉普拉斯方法较优。</p>
<h4 id="总结和思考"><a href="#总结和思考" class="headerlink" title="总结和思考"></a>总结和思考</h4><p>Make3D方法的经典不用多说，值得思考的是算法中应用全局特征的方法，近期的研究如多尺度CNN等也用多尺度来表示一种全局方法，但是要想接近人类的深度识别，我们不光要考虑到局部特征和全局特征，还要考虑到过往经验和知识。因此，如何将知识应用到深度识别中去是另一个值得研究的方向。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/" data-id="cjhpuzab5000ceg8uh793pget" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Monocular-Depth-Estimation-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/07/Monocular-Depth-Estimation-2/" class="article-date">
  <time datetime="2018-02-07T02:27:41.000Z" itemprop="datePublished">2018-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/07/Monocular-Depth-Estimation-2/">Monocular Depth Estimation 2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<h3 id="单目图像深度估计-2-应用篇：Learning-to-be-a-Depth-Camera"><a href="#单目图像深度估计-2-应用篇：Learning-to-be-a-Depth-Camera" class="headerlink" title="单目图像深度估计 - 2. 应用篇：Learning to be a Depth Camera"></a>单目图像深度估计 - 2. 应用篇：Learning to be a Depth Camera</h3><p><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth2_paper1.png" alt="Learning to be a Depth Camera"><br>Fig. 1 Learning to be a Depth Camera</p>
<p>第二篇借助Learning to be a Depth Camera for Close-Range Human Capture and Interaction（<a href="https://www.microsoft.com/en-us/research/publication/learning-to-be-a-depth-camera-for-close-range-human-capture-and-interaction/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2Fdefault.aspx%3Fid%3D220845" target="_blank" rel="noopener">Website</a>）这篇论文谈一下相关应用。</p>
<p>上篇文章中提到过，有了深度信息后，图片就从二维变成了三维，机器对图像的理解成几何倍数的增长，自然利用这更多的信息可以做到更多的事情。<br>当机器人可以实时获取空间的三维信息，当人机交互只需要手势和表情，当智能汽车可以实时推断周围物体距离……</p>
<p>Learning to be a Depth Camera这篇论文提供了一种把普通摄像头改造成深度摄像头的方法，结合<strong>硬件的改造</strong>和<strong>机器学习算法</strong>推断可实时获取近距离物体的深度信息。在论文配套的<strong><a href="https://dl.acm.org/citation.cfm?id=2601185&amp;dl=ACM&amp;coll=DL" target="_blank" rel="noopener">视频</a></strong>中展示了多种应用，如人脸3D建模，手势识别等。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth2_paper4.png" alt="Learning to be a Depth Camera"><br>Fig. 2 不同应用场景<br>之所以把这篇叫做应用篇，也是因为在视频中给出的应用场景非常丰富，并且能够直观的深度信息看出在人机交互和3D建模方面的重要性。</p>
<h4 id="硬件改造部分"><a href="#硬件改造部分" class="headerlink" title="硬件改造部分"></a>硬件改造部分</h4><p>文中对两种摄像头进行了改造，分别是普通的电脑外接摄像头Microsoft LifeCam(Fig.1 d)和Nexus手机摄像头(Fig.1 c)。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth2_paper2.png" alt="Camera"><br>Fig. 3 摄像头改造步骤<br>如上图，改造的步骤为：</p>
<ol>
<li>去掉相机原有的近红外滤光片(NIR cut filter)</li>
<li>增加一个带通滤波片(bandpass filter)</li>
<li>增加一圈红色LED光源<br>其中带通滤波片带宽和新增的LED光源波长均为850nm。<br>在视频中有详细的改造过程，但此处值得注意的是，目前手机相机镜头可能不是单独的带通滤波片结构，而是一层滤波涂层，因此效果可能会打折。<br>这种改造的目的是让原本不能感知近红外的镜头对特定波长的近红外光波敏感，改造后的相机输出结果不再为彩色RGB图片。</li>
</ol>
<h4 id="深度估计算法部分"><a href="#深度估计算法部分" class="headerlink" title="深度估计算法部分"></a>深度估计算法部分</h4><p>文章使用机器学习的方法对深度信息进行估计。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth2_paper3.png" alt="Multi-layer decision forest"><br>Fig. 4 多层决策森林<br>Fig.4为文章进行深度估计使用的多层决策森林结构，输入为一个红外图像的像素点，输出为深度距离值。网络为两层结构，第一层为分类森林，第二层为回归森林。为了让预测过程更有效率，文章限制了目标物体的距离范围(20cm到1m)。<br>如图所示，Layer1为每一个像素分配一块对应的深度范围（每一个叶子节点表示一个深度范围），并且生成整幅图片的预测深度概率分布（计算分别属于c个叶子的概率）。由于叶子节点的输出并不是具体的深度值，且输出中有全局概率分布，因此第一层可理解为全局的粗略深度估计。<br>此处插播一句，深度特征不同于其他特征的地方有一点就在于深度信息是全局性的，是一种相对的值，人类也会通过相对距离来判断深度，因此大多数深度估计方法都采用一种由粗到细由全局到局部的预测过程，典型的例子就是多尺度深度网络。</p>
<p>Layer2针对layer1的c种分类结果，训练了c个对应的专家树，每一个专家树都由属于此范围的train集进行训练得到。通过激活不同的专家树，并赋予每个（被激活的专家树的）预测结果对应的权重（权重由Layer1给的概率分布决定），获得每个像素点最终的深度预测值。第二层的输出是mm单位的连续深度预测值。<br>值得注意的是，论文中提到可以使用Kinect获得的真实深度数据对模型进行训练，在应用时使用改造后的摄像头数据，也可以实时产生令人满意的效果。</p>
<p>由于决策树这块我还没吃透，再此不对训练过程做详细说明，大致是通过最大化信息增益得到每个节点的判断阈值的过程。实验对比部分也一并略过。</p>
<h4 id="不足及展望部分"><a href="#不足及展望部分" class="headerlink" title="不足及展望部分"></a>不足及展望部分</h4><p>本文提出的方法软硬结合的解决了单目摄像头深度估计问题，优点有成本低廉，实时性强，不需要手动提取特征等，缺点有如下几点：</p>
<ol>
<li>不适用于表面介质不同的物体。文中作对比的SFS方法是基于物体的反射特等物理特性进行深度推测的方法，因此作者强调了此种方法只适用于均一介质的人的皮肤，在视频中也有对塑料模型建模失败的效果展示。</li>
<li>需要大量覆盖不同情况的train set。这可能是机器学习和深度学习方法的通病了，过于依赖train集，如用室外数据集KITTI训练得到的深度网络在室内数据集NYU_Depth上的表现往往不尽如人意。</li>
<li>只能探测近距离的物体。由于对距离进行了限制（20cm到1m），但也正是因此可以忽略背景的干扰降低了算法复杂度，更适合于人机交互的应用。</li>
</ol>
<p>作者也提出了可以改进的方向，如利用已有可直接获取红外值的Camera(Omni Vision)获取更精确的结果。</p>
<h4 id="总结和思考"><a href="#总结和思考" class="headerlink" title="总结和思考"></a>总结和思考</h4><p>这篇文章值得学习的地方有很多，比如引人入胜的视频，通过视频可以看出作者团队做了多少工作；比如丰富的实验，不仅仅横向与其他算法对比，而且与不同的训练方式对比，同时在论文的结尾还对手部区域分类的应用进行了详细的实验说明；比如软硬结合的方法，在工业应用上这篇文章比其他单独对算法进行改进的论文无疑更有实用价值。</p>
<p>最近一直在思考论文的价值，是发表了论文就算有价值吗？为了提高百分之零点几的正确率让算法变得更加复杂真的是正确的研究方向吗？我个人还是比较喜欢简洁的方法，简洁即优雅，就像Science上的CFSFDP一样，提出全新的思路和方向的论文对我来说更有意思一些。<br>以上。</p>
<p>[1] Fanello S R, Keskin C, Izadi S, et al. Learning to be a depth camera for close-range human capture and interaction[J]. Acm Transactions on Graphics, 2014, 33(4):1-11.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/" data-id="cjhpuzab30009eg8uvxy078l1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Monocular-Depth-Estimation-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/05/Monocular-Depth-Estimation-1/" class="article-date">
  <time datetime="2018-02-05T08:08:07.000Z" itemprop="datePublished">2018-02-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/Monocular-Depth-Estimation-1/">Monocular Depth Estimation - 1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="单目图像深度估计-1-入门篇"><a href="#单目图像深度估计-1-入门篇" class="headerlink" title="单目图像深度估计 - 1.入门篇"></a>单目图像深度估计 - 1.入门篇</h3><p>最近一直在看单目深度图像估计相关的Paper，小白入门困难多多，于是打算把看过的几篇论文和相关理论总结一下。<br>顺序如下：</p>
<ol>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">入门篇：图像深度估计相关总结</a></li>
<li><a href="https://qunniekong.github.io/2018/02/07/Monocular-Depth-Estimation-2/">应用篇：Learning to be a Depth Camera</a></li>
<li><a href="https://qunniekong.github.io/2018/02/22/Monocular-Depth-Estimation-3/">尺度篇：Make3D</a></li>
<li><a href="https://qunniekong.github.io/2018/03/06/Monocular-Depth-Estimation-4/">迁移篇：Depth Extraction from Video Using Non-parametric Sampling</a></li>
<li><a href="https://qunniekong.github.io/2018/05/22/Monocular-Depth-Estimation-5/">深度篇：David Eigen</a></li>
<li><a href="https://qunniekong.github.io/2018/05/27/Monocular-Depth-Estimation-6/">无监督篇：Left-Right Consistency &amp; Ego Motion</a></li>
<li><a href="https://qunniekong.github.io/2018/05/28/Monocular-Depth-Estimation-7/">相对深度篇：Depth in the Wild &amp; Size to Depth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">SLAM辅助篇：MegaDepth</a></li>
<li><a href="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/">方法比较篇：Evaluation of CNN-based Methods</a></li>
</ol>
<hr>
<p>作为入门篇，这篇写的大部分是我个人的想法，脉络的话大约按照是什么，为什么和怎么做三部分进行。</p>
<h4 id="什么是图像深度估计"><a href="#什么是图像深度估计" class="headerlink" title="什么是图像深度估计"></a>什么是图像深度估计</h4><p>顾名思义，深度估计就是从RGB图像中估计图像中物体的深度，是一个从二维到三维的艰难过程。此处的艰难是对计算机来说，而人类的视觉系统是天生的双目系统，并且通过大脑的计算可以实时生成深度信息甚至空间的三维建模。</p>
<p>所谓双目立体视觉，即模仿人眼成像原理，在同一时刻不同位置用两台相同的设备对物体进行观测。光沿直线传播，因此同一物体在不同位置的成像可最终确定物体的真实位置。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth1_stereo1.png" alt="双目立体视觉"><br>在两个相机已经校准的前提下，已知相机的光点距离、焦距和物体在画面内的水平距离等参数时，可计算得到真实距离信息。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth1_stereo2.png" alt="双目立体视觉计算方法"><br>双目立体视觉是目前常见的深度信息获取方法，在SLAM、智能车、体感游戏等领域都有大规模的应用。其缺点也较为明显，如硬件体积小，计算量大（图像校准），精度受限（复杂重复画面如树林影响图像校准，时间轴校准困难）等。目前常见的深度信息获取方法除双目立体视觉方法外还有结构光、TOF等方法（见下图)，但各有优缺点，比如体积大（TOF）、能耗高（Kinect配有散热系统）、受环境影响（阳光中红外线影响）、算法复杂度高、实时性差（TOF实时性最高但精度较低）等。<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth1_method.png" alt="深度图像获取方法"></p>
<p>由于以上基于硬件的深度获取方法还存在一定缺点，在深度学习、机器学习、人工智能飞速发展的今天，用更“智能”的方法对图像深度进行估计，弥补硬件的不足，同时为其他图像应用如语义分割、物体识别等提供更多的特征信息成了大牛们研究的方向。</p>
<h4 id="为什么是单目图像深度估计"><a href="#为什么是单目图像深度估计" class="headerlink" title="为什么是单目图像深度估计"></a>为什么是单目图像深度估计</h4><p>无论是深度学习、机器学习还是机器视觉的目标，到目前为止仍然是：更接近人类。那么有趣的问题来了，就算我们闭上一只眼睛（神盾局长、海盗船长）仍然可以分辨物体深度。我们是怎么做到的？从眼睛的成像原理来看，人类获取到的也只是某一时刻物体的二维成像，我们没有类似深度传感器的结构，那么深度信息是什么时候附加在我们看到的画面上的？<br>因为我们的大脑利用了已有的知识：<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth1_visual1.png" alt="深度知识"><br>近大远小<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth1_visual2.png" alt="深度知识"><br>推断感知(?)<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/depth1_visual3.png" alt="深度知识"><br>消失点</p>
<p>这些透视知识学习过绘画的人会更了解，我们的大脑总结出了这些知识和经验，并对每次观测附加相应的信息，形成了精度极高的深度估计系统，在捂住一只眼睛的情况下，也可以对深度进行相应的估计和预测。<br>很有趣吧。</p>
<p>无论是深度学习还是机器学习，核心都是学习，我们希望机器能拥有像人脑一样的学习能力，所谓学习就是总结并利用知识和经验的能力。因此，若是能训练机器完成对单幅图像的深度估计，那么在知识的理解和运用上也是一种突破，更别提单目相比传统深度获取方法的优点（成本低，体积小，能耗低等）了。<br>并且单目图像深度估计的应用广泛，常见的有电影2D转3D，网络图片理解，3D建模，机器人，智能车等。</p>
<h4 id="怎么进行单目图像深度估计"><a href="#怎么进行单目图像深度估计" class="headerlink" title="怎么进行单目图像深度估计"></a>怎么进行单目图像深度估计</h4><p>由于我刚刚进行这方面的研究，只看了不到10篇论文，这几篇论文的方法大致可以分为：基于深度迁移的方法，基于相对深度的方法和基于深度网络的方法三类。在之后的文章中会详细讲解每一篇论文。<br>毫无疑问，随着深度学习方法的发展，应用深度网络进行深度估计越来越成为主流的单目图像深度估计方法，但这种越来越复杂化的方法真的是正确的发展方向吗？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/02/05/Monocular-Depth-Estimation-1/" data-id="cjhpuzab10008eg8uo2avttl4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-SSH-for-Windows" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/02/SSH-for-Windows/" class="article-date">
  <time datetime="2018-01-02T02:32:35.000Z" itemprop="datePublished">2018-01-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/02/SSH-for-Windows/">SSH for Windows</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Windows7进行SSH连接"><a href="#Windows7进行SSH连接" class="headerlink" title="Windows7进行SSH连接"></a>Windows7进行SSH连接</h3><p>最近尝试在GPU服务器上运行深度网络的推断，因为我的电脑为Windows 7系统，所以用到了Windows和Linux进行SSH通信的一些方法，在此记录一下。</p>
<ol>
<li>连接服务器<ul>
<li>Windows进行SSH连接需要使用特定软件，我下载的是PUTTY，一款小巧的软件，可以建立连接，连接成功后界面相当于Linux的控制台。</li>
<li>连接方法：<br>  打开PUTTY，输入服务器的地址，默认SSH端口为22，点击open后提示输入用户名，回车之后输入密码即可。</li>
<li>常用命令：<br>  记几个Linux中常用的命令：<br>  1) pwd —— 显示当前地址<br>  2) cd 地址 —— 地址跳转<br>  3）ifconfig —— 显示网络信息<br>  4）rm 文件名 —— 删除文件<br>  5）mkdir —— 创建地址、文件夹<br>  6）ls —— 显示当前目录下的所有文件及文件夹<br>  7）nvidia-sim —— 显示显卡使用情况</li>
</ul>
</li>
<li>上传、下载文件<ul>
<li>文件操作需要使用PUTTY中的PSFTP软件，打开软件以后输入指令open 服务器地址，系统提示输入用户名，回车后输入密码则显示连接成功。文件上传和下载时需要注意确定当前路径是否正确。</li>
<li>上传文件：<br>  1) 上传文件夹：需要-r表示递归<pre><code>put -r c:/Users/Desktop/abc —— abc为文件夹名
</code></pre>  2) 上传文件：<pre><code>put c:/Users/Desktop/a.jpg
</code></pre></li>
<li>下载文件：<br>  1) 首先需要设定下载到本地的存储目录：<pre><code>lcd c:/Users/Desktop
</code></pre>  2）get 文件名</li>
</ul>
</li>
<li>注意事项<ul>
<li>如果两次上传的文件名相同，则默认覆盖已有文件且不进行报错，可能会造成文件损失</li>
<li>下载文件时如果不指定本地位置(lcd指令)则会报unable to open错误</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2018/01/02/SSH-for-Windows/" data-id="cjhpuzabg000jeg8ugi30w59z" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SSH/">SSH</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Install-OpenCV-on-Raspberry-Pi" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/26/Install-OpenCV-on-Raspberry-Pi/" class="article-date">
  <time datetime="2017-12-26T09:45:46.000Z" itemprop="datePublished">2017-12-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/26/Install-OpenCV-on-Raspberry-Pi/">Install OpenCV on Raspberry Pi</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="树莓派上OpenCV的安装"><a href="#树莓派上OpenCV的安装" class="headerlink" title="树莓派上OpenCV的安装"></a>树莓派上OpenCV的安装</h3><p>  想在树莓派上链接手机摄像头作为IP摄像头在树莓派上实现人脸识别，于是开始了漫长的OpenCV安装之路。<br>  参考资料：</p>
<ul>
<li><a href="https://www.jianshu.com/p/67293b547261" target="_blank" rel="noopener">翻译 Python 2.7 和 Python 3+ 的OpenCV 3.0 安装教程</a></li>
<li><a href="http://blog.csdn.net/talkxin/article/details/50471986" target="_blank" rel="noopener">在树莓派B+上编译安装opencv 3.1.0 for both python</a></li>
<li><a href="https://www.pyimagesearch.com/2016/04/18/install-guide-raspberry-pi-3-raspbian-jessie-opencv-3/" target="_blank" rel="noopener">Install guide: Raspberry Pi 3 + Raspbian Jessie + OpenCV 3</a></li>
</ul>
<h4 id="Rasberry-Pi-Python3-OpenCV-3-0-0"><a href="#Rasberry-Pi-Python3-OpenCV-3-0-0" class="headerlink" title="Rasberry Pi(Python3) + OpenCV 3.0.0"></a>Rasberry Pi(Python3) + OpenCV 3.0.0</h4><pre><code>sudo apt-get update
sudo apt-get upgrade
sudo rpi-update
sudo apt-get install build-essential git cmake pkg-config
sudo apt-get install libjpeg8-dev libtiff5-dev libjasper-dev libpng12-dev
sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
sudo apt-get install libgtk2.0-dev
sudo apt-get install libatlas-base-dev gfortran
cd ~
git clone https://github.com/Itseez/opencv.git
cd opencv
git checkout 3.0.0
cd ~
git clone https://github.com/Itseez/opencv_contrib.git
cd opencv_contrib
git checkout 3.0.0
sudo apt-get install python3-dev
sudo python3 get-pip.py
sudo pip3 install virtualenv virtualenvwrapper
sudo rm -rf ~/.cache/pip
nano ~/.profile
export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3
export WORKON_HOME=$HOME/.virtualenvs
source /usr/local/bin/virtualenvwrapper.sh
source ~/.profile
mkvirtualenv p3cv
workon p3cv
pip3 install numpy
# 若报没有权限的错误执行sudo rm -rf ~/.cache/pip/
cd ~/opencv
mkdir build 
cd build
cmake -D CMAKE_BUILD_TYPE=RELEASE \
-D CMAKE_INSTALL_PREFIX=/usr/local \
-D INSTALL_C_EXAMPLES=OFF \
-D INSTALL_PYTHON_EXAMPLES=ON \
-D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \
-D BUILD_EXAMPLES=ON ..
# 检查python环境是否正常，ImportError：No module named cv2则检查.profile加入export LD_LIBRARY_PATH=/usr/lib/:$LD_LIBRARY_PATH
make -j4
sudo make install
sudo ldconfig
cd ~/.virtualenvs/p3cv/lib/python3.4/site-packages/
ln -s /usr/local/lib/python3.4/site-packages/cv2.so cv2.so
</code></pre><h4 id="有用的指令"><a href="#有用的指令" class="headerlink" title="有用的指令"></a>有用的指令</h4><ol>
<li><p>更改管理员权限的密码：</p>
<p> sudo passwd root<br> sudo passwd –unlock root</p>
</li>
<li><p>设置代理服务器<br> 1)打开profile文件</p>
<pre><code>vi /etc/profile
</code></pre><p> 2) 添加如下语句</p>
<pre><code>http_proxy=1.1.1.1:8080
no_proxy=10.2.44.44
export http_proxy no_proxy
</code></pre><p> 3) 打开文件指令为vi，打开后按insert键开始编辑，编辑完成按esc键输入:wq保存（或输入:q!不保存直接关闭），执行文件指令为source</p>
<pre><code>source /etc/profile
</code></pre></li>
<li><p>进入虚拟环境</p>
<p> source /usr/local/bin/virtualenvwrapper.sh<br> workon p3cv</p>
</li>
</ol>
<h4 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h4><ol>
<li>ImportError：No module named cv2:将cv2.so文件复制在各种文件夹内</li>
<li>忘记是什么问题，解决方法是把ffe…的文件复制到python文件夹内</li>
<li>目前仍然没有解决的问题是树莓派无法连接公司的代理服务器上外网</li>
<li>环境变量配置不成功，每次需要手动进入虚拟环境</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2017/12/26/Install-OpenCV-on-Raspberry-Pi/" data-id="cjhpuzaat0003eg8uaqa94ixv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Tensorflow-Install-With-Anaconda" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/13/Tensorflow-Install-With-Anaconda/" class="article-date">
  <time datetime="2017-12-13T08:04:28.000Z" itemprop="datePublished">2017-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/13/Tensorflow-Install-With-Anaconda/">Tensorflow install with Anaconda</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="在Anaconda的python环境中安装TensorFlow"><a href="#在Anaconda的python环境中安装TensorFlow" class="headerlink" title="在Anaconda的python环境中安装TensorFlow"></a>在Anaconda的python环境中安装TensorFlow</h3><ol>
<li>安装Anaconda</li>
<li>创建python3.5环境<ul>
<li>cmd中conda create -n tensorflow python=3.5，其中tensorflow为环境的名称</li>
<li>目前TensorFlow对python3.5的支持最好</li>
<li>如果无法使用conda命令则需要手动添加环境变量：path中增加Anaconda3\Script路径<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/tensorflow%E9%85%8D%E7%BD%AE1.png" alt="Anaconda Python环境搭建"></li>
</ul>
</li>
<li>安装TensorFlow<ul>
<li>Activate tensorflow：启动tensorflow环境</li>
<li>Pip install tensorflow<br>  ** 在有代理的网络下需要设置pip代理服务器：pip install –proxy(anaconda环境貌似不需要)</li>
<li>deactivate：退出环境</li>
</ul>
</li>
<li>配置PyCharm<ul>
<li>File-&gt;Settings-&gt;Project:plug-n-learn-&gt;Project Interpreter 选择Anaconda目录中env（虚拟环境）下的 环境名称\python.exe</li>
<li>pycharm包管理：想用pycharm下载更新包需要设置代理服务器（设置方法为 file-settings-appearance &amp; behavior-system settings-HTTP Proxy）<br><img src="https://raw.githubusercontent.com/QunnieKong/UsefullCodePics/master/HexoPics/tensorflow%E9%85%8D%E7%BD%AE2.png" alt="Pycharm配置"></li>
</ul>
</li>
<li>配置spyder<ul>
<li>打开anaconda navigator，下拉菜单选择tensorflow环境后点击spyder图标下的install按钮</li>
</ul>
</li>
<li>测试<ul>
<li>activate tensorflow</li>
<li>python</li>
<li>import tensorflow</li>
<li>成功</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qunniekong.github.io/2017/12/13/Tensorflow-Install-With-Anaconda/" data-id="cjhpuzabu000peg8u0x75b5dk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog相关/">Blog相关</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Monocular-Depth-Estimation/">Monocular Depth Estimation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SSH/">SSH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/移动端开发/">移动端开发</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Blog相关/" style="font-size: 15px;">Blog相关</a> <a href="/tags/Monocular-Depth-Estimation/" style="font-size: 20px;">Monocular Depth Estimation</a> <a href="/tags/SSH/" style="font-size: 10px;">SSH</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/移动端开发/" style="font-size: 15px;">移动端开发</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/05/28/Monocular-Depth-Estimation-7/">Monocular Depth Estimation 7</a>
          </li>
        
          <li>
            <a href="/2018/05/27/Monocular-Depth-Estimation-6/">Monocular Depth Estimation 6</a>
          </li>
        
          <li>
            <a href="/2018/05/22/Monocular-Depth-Estimation-5/">Monocular Depth Estimation 5</a>
          </li>
        
          <li>
            <a href="/2018/03/06/Monocular-Depth-Estimation-4/">Monocular Depth Estimation 4</a>
          </li>
        
          <li>
            <a href="/2018/02/22/Monocular-Depth-Estimation-3/">Monocular Depth Estimation 3</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 KDQ<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>